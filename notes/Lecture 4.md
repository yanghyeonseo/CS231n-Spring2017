# Lecture 4 | Backpropagation and Neural Networks

# Computational graphs

ì„ì˜ì˜ ë³µì¡í•œ í•¨ìˆ˜ì— ëŒ€í•´ analytic gradientë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²• â†’ backpropagation ì‚¬ìš© ê°€ëŠ¥

backpropagation: chain ruleì„ ì¬ê·€ì ìœ¼ë¡œ ì´ìš©í•´ì„œ computational graph ë‚´ ëª¨ë“  ë³€ìˆ˜ì— ëŒ€í•œ gradient ê³„ì‚°

â‡’ convolutional neural networksì™€ ê°™ì€ ë³µì¡í•œ í•¨ìˆ˜ì—ì„œ ìœ ìš© (+ ë‹¤ë¥¸ ë”¥ëŸ¬ë‹ ëª¨ë¸ì¸ Neural Turing Machineì—ì„œë„ ì‚¬ìš©)

1. functionì„ computational graphë¡œ í‘œí˜„
2. ê° ë³€ìˆ˜ì— ê°’ì„ ëŒ€ì…í•˜ê³ , ê° ë…¸ë“œì˜ gradientì™€ intermediate valueë¥¼ ê³„ì‚°
3. ë§ˆì§€ë§‰ ë…¸ë“œë¶€í„° ì¬ê·€ì ìœ¼ë¡œ fì˜ í¸ë¯¸ë¶„ì„ ê³„ì‚°. (ì§ì ‘ì ìœ¼ë¡œ fì™€ ë³€ìˆ˜ê°€ ì—°ê´€ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´ chain rule í™œìš©)

<aside>
ğŸ’¡

ê° ë…¸ë“œëŠ” local gradientë¥¼ ê°€ì§€ê³  ìˆìŒ.

**backpropagationì„ í†µí•´ upstream gradient ê°’ì„ ë°›ìœ¼ë©´ ì´ë¥¼ local gradientì™€ ê³±í•˜ì—¬ ì—°ê²°ëœ ë’· ë…¸ë“œì—ê²Œ ì „ë‹¬í•œë‹¤**

(ë‘ ê°œ ì´ìƒì˜ ë…¸ë“œì—ì„œ upstream gradient ê°’ì„ ë°›ìœ¼ë©´ ì´ë“¤ì„ ëª¨ë‘ ë”í•˜ì—¬ total upstream gradientë¡œ ê³„ì‚°)

</aside>

â€» computational nodeëŠ” ì•„ë¬´ ë‹¨ìœ„ë¡œë‚˜ ì„¤ì •í•  ìˆ˜ ìˆìŒ
â‡’ ë§ì…ˆ, ê³±ì…ˆ ë“± ê°€ì¥ ë‹¨ìˆœí•œ ì—°ì‚°ë¶€í„° ì—¬ëŸ¬ ì—°ì‚°ì„ ë™ì‹œì— ì§„í–‰í•˜ëŠ” ê²ƒê¹Œì§€ (ì˜ˆ, sigmoid function)

- Patterns in backward flow
    - add gate: gradient distributor
    - max gate: gradient router
    - mul gate: gradient switcher (and scaler)

â†’ ê°’ì„ ëª¨ë“  ë’· ë…¸ë“œë¡œ

â†’ ê°’ì„ ë” í° ë…¸ë“œë¡œë§Œ (ë‚˜ë¨¸ì§€ 0)

â†’ ê°’ì„ ë‹¤ë¥¸ branchì˜ ê°’ìœ¼ë¡œ scale

<aside>
ğŸ’¡

ë³€ìˆ˜ë“¤ì´ vectorë¼ë©´? ëª¨ë“  gradientë“¤ì„ Jacobian matrixë¡œ í‘œí˜„

</aside>

### Modularized implementation

- forward API â†’ nodeì˜ output ê³„ì‚°
- backward API â†’ gradient ê³„ì‚°

```python
class ComputationalGraph(object):
    #...

    def forward(inputs):
        # 1. [pass inputs to input gates...]
        # 2. forward the computational graph:
        for gate in self.graph.nodes_topologically_sorted():
            gate.forward()
        return loss  # the final gate in the graph outputs the loss

    def backward():
        for gate in reversed(self.graph.nodes_topologically_sorted()):
            gate.backward()  # little piece of backprop (chain rule applied)
        return inputs_gradients
```

- gate
    - input ê°’ì„ cache í•´ì•¼ í•¨

```python
class MultiplyGate(object):
    def forward(self, x, y):
        z = x * y
        self.x = x  # must keep these around!
        self.y = y
        return z

    def backward(self, dz):
        dx = self.y * dz  # [dz/dx * dL/dz]
        dy = self.x * dz  # [dz/dy * dL/dz]
        return [dx, dy]
```

â†’ frameworkì— layerë¡œ êµ¬í˜„ë˜ì–´ ìˆìŒ

## Summary

- neural netsëŠ” ë§¤ìš° í¬ê¸° ë•Œë¬¸ì— gradient formulaë¥¼ ì§ì ‘ ì‘ì„±í•˜ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥
- backpropagation: computational graphë¥¼ ë”°ë¼ chain ruleì„ ì¬ê·€ì ìœ¼ë¡œ ì ìš©í•¨ìœ¼ë¡œì¨ ëª¨ë“  ì…ë ¥/ë§¤ê°œë³€ìˆ˜/ì¤‘ê°„ê°’ì˜ gradientë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆìŒ
    - forward: ì—°ì‚°ì˜ ê²°ê³¼ë¥¼ ê³„ì‚°í•˜ê³  ì¤‘ê°„ê°’ì„ ì €ì¥
    - backward: chain rule ì ìš©í•˜ì—¬ inputì— ëŒ€í•œ loss functionì˜ gradient ê³„ì‚°

# Neural Networks

- (Before) Linear score function
    
    $$
    f = Wx
    $$
    
- (Now) 2-layer Neural Network
    - ì²« ë²ˆì§¸ layer = non-linear, ë‘ ë²ˆì§¸ layer = linear
    - í•˜ë‚˜ì˜ template ë°–ì— ì—†ë˜ ë¬¸ì œ
    multiple layer â†’ W1ì€ template, W2ëŠ” weighted sum of templates
    ~~ê° intermediate variableê°€ templateì„ ê°€ì§ˆ ìˆ˜ ìˆìŒ~~
    
    $$
    f = W_2\max\left(0, W_1x\right)
    $$
    
- (â€¦) 3-layer Neural Network
    
    $$
    f = W_3\max\left(0, W_2\max\left(0, W_1x\right)\right)
    $$
    

â†’ Neural networkëŠ” ë³µì¡í•œ non-linear functionì„ í˜•ì„±í•˜ê¸° ìœ„í•´ hierarchically ìŒ“ì¸ class of functions

### Neuron

Neuronê³¼ computational graphì˜ nodeëŠ” ì‘ë™ ë°©ì‹ì´ ë¹„ìŠ·í•¨

| ë‰´ëŸ° | ë…¸ë“œ | ì°¨ì´ |
| --- | --- | --- |
| ê°€ì§€ëŒê¸° | ì•ì„  ë…¸ë“œë¡œë¶€í„°ì˜ ê°€ì§€ | ê°€ì§€ëŒê¸°ëŠ” ë³µì¡í•œ ë¹„ì„ í˜• ì—°ì‚° ê°€ëŠ¥ |
| ì„¸í¬ì²´ | ì—°ì‚° + activation function | ì‹œëƒ…ìŠ¤ëŠ” í•˜ë‚˜ì˜ ê°€ì¤‘ì¹˜ê°€ ì•„ë‹Œ ë³µì¡í•œ ë¹„ì„ í˜• ë™ì  ì‹œìŠ¤í…œ |
| ì¶•ìƒ‰ëŒê¸° | ë’¤ ë…¸ë“œë¡œì˜ ê°€ì§€ | rate code ì¶©ë¶„íˆ ì„¤ëª… ëª»í•¨ |
- ë‰´ëŸ°ì€ ê°€ì§€ëŒê¸°(dendrite)ë¡œ ìê·¹(impulse)ì„ ì „ë‹¬ë°›ê³ , ë…¸ë“œë„ ì•ì„  ë…¸ë“œë¡œë¶€í„° ì…ë ¥ì„ ì „ë‹¬ë°›ìŒ
- ë‰´ëŸ°ì€ ì´ ìê·¹ì„ ì„¸í¬ì²´(cell body)ì—ì„œ í†µí•©í•˜ê³ , ë…¸ë“œë„ ì—°ì‚°ê³¼ activation functionì„ í†µí•´ ê³„ì‚°í•¨
    - ë‰´ëŸ°ì˜ ìê·¹ì€ ê¸‰ë“±(spike)ì˜ ëª¨ì–‘ì„ ë„ëŠ”ë° activation functionì´ ì •í™•íˆ ê·¸ ì—­í• ì„ í•¨ (ì˜ˆ, sigmoid activation function)
- ë‰´ëŸ°ì€ ì¶•ìƒ‰ëŒê¸°(axon)ìœ¼ë¡œ ìê·¹ì„ ì „ë‹¬í•˜ê³ , ë…¸ë“œë„ ì¶œë ¥ì„ ë’¤ì˜ ë…¸ë“œë¡œ ì „ë‹¬

### Neural Networks

- ê° layerëŠ” matrix multiply
- layer(ë˜ëŠ” hidden layer) ìˆ˜ì— ë”°ë¼ â€˜[n]-layer Neural Netâ€™ ë˜ëŠ” â€˜[n-1]-hidden-layer Neural Netâ€™

```python
def neuron_tick(inputs):
    # assume inputs and weights are 1-D numpy arrays and bias is a number
    cell_body_sum = np.sum(inputs * self.weights) + self.bias
    firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum))  # sigmoid activation
    return firing_rate
```