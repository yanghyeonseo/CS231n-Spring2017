# Lecture 5 | Convolutional Neural Networks

# History

- Frank Rosenblatt, ~1957: Perceptron
ìµœì´ˆì˜ í¼ì…‰íŠ¸ë¡ (ë‘ë‡Œì˜ ì¸ì§€ ëŠ¥ë ¥ì„ ëª¨ë°©í•˜ë„ë¡ ë§Œë“  ì¸ìœ„ì ì¸ ë„¤íŠ¸ì›Œí¬) ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
â†’ f(x) = 0 or 1, update rule ì¡´ì¬
- Widrow and Hoff, ~1960: Adaline/Madaline
ì—¬ëŸ¬ linear classifierë¥¼ ì—°ê²°
- Rumelhart et al., 1986: First time back-propagation became popular
ìš°ë¦¬ì—ê²Œ ì¹œìˆ™í•œ back-propagation ë“±ì¥
- Hinton and Salakhutdinov 2006: Reinvigorated research in Deep Learning
initialize ê³¼ì •ì´ ë³µì¡í•˜ì§€ë§Œ backpropì„ ì´ìš©í•œ ìµœì´ˆì˜ íŒŒì¸íŠœë‹
- Acoustic Modeling using Deep Belief Networks, Abdel-rahman Mohamed, George Dahl, Geoffrey Hinton, 2010
Context-Dependent Pre-trained Deep Neural Networks for Large Vocabulary Speech Recognition, George Dahl, Dong Yu, Li Deng, Alex Acero, 2012
Imagenet classification with deep convolutional
neural networks, Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, 2012
ìŒí–¥, ì´ë¯¸ì§€ì— ëŒ€í•œ ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ê°•ë ¥í•œ ê²°ê³¼

- Hubel & Wiesel
1959: RECEPTIVE FIELDS OF SINGLE NEURONES IN THE CAT'S STRIATE CORTEX
1962: RECEPTIVE FIELDS, BINOCULAR INTERACTION AND FUNCTIONAL ARCHITECTURE IN THE CAT'S VISUAL CORTEX
1968â€¦
ê³ ì–‘ì´ê°€ ì‹œê°ì  ìê·¹ì„ ë°›ì•˜ì„ ë•Œ í”¼ì§ˆì˜ ë‰´ëŸ°ì˜ ë°˜ì‘ì„ ê´€ì°°
- Topographical mapping in the cortex: nearby cells in cortex represent nearby regions in the visual field
í”¼ì§ˆì˜ ì¸ì ‘í•œ ì„¸í¬ê°€ ì‹œì•¼ì—ì„œ ì¸ì ‘í•œ êµ¬ì—­ì— í•´ë‹¹í•œë‹¤
- Hierarchical organization
ë§ë§‰ ì ˆì„¸í¬(Retinal ganglion cell)ëŠ” ìˆ˜ìš©ì•¼, ë‹¨ìˆœ ì„¸í¬(Simple cell)ëŠ” ë¹›ì˜ ë°©í–¥, ë³µí•© ì„¸í¬(Complex cell)ëŠ” ë¹›ì˜ ë°©í–¥ ë° ì›€ì§ì„, ê³¼ë³µí•© ì„¸í¬(Hypercomplex cell)ëŠ” ì›€ì§ì„ê³¼ ì¢…ì ì— ë°˜ì‘í•œë‹¤
- Neocognitron [Fukushima 1980]
â€œsandwichâ€ architecture (SCSCSCâ€¦)
simple cells: modifiable parameters
complex cells: perform pooling
ë‹¨ìˆœ ì„¸í¬ì™€ ë³µí•© ì„¸í¬ë¥¼ ìƒŒë“œìœ„ì¹˜ì²˜ëŸ¼ ë°°ì¹˜í•˜ì—¬ ë‹¨ìˆœ ì„¸í¬ì—ì„œì˜ ì‘ì€ ë³€í™”ì— ëŒ€í•´ invariant
- Gradient-based learning applied to document recognition [LeCun, Bottou, Bengio, Haffner 1998]
backpropagationê³¼ gradient ê¸°ë°˜ í•™ìŠµì„ Convolutional Neural Networkë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ë°ì— ì ìš©, ìˆ«ìë¥¼ ì¸ì‹í•˜ëŠ” ë° ì˜ ë™ì‘
- ImageNet Classification with Deep Convolutional Neural Networks [Krizhevsky, Sutskever, Hinton, 2012]
â€AlexNetâ€ = í˜„ëŒ€ì˜ Convolutional Neural Network
LeCunì˜ ê²ƒê³¼ í¬ê²Œ ë‹¤ë¥´ì§€ ì•Šì§€ë§Œ ë°ì´í„°ì™€ ë³‘ë ¬ ì»´í“¨íŒ…(GPU)ì˜ ë°œì „ í™œìš©
- í˜„ì¬: ConvNets are everywhere

# Convolutional Neural Networks (w/o the brain stuff)

## Convolutional Layer

Fully Connected Layerì™€ ë‹¤ë¥´ê²Œ ê³µê°„ êµ¬ì¡°(spatial structure)ë¥¼ ìœ ì§€í•¨

![image.png](assets/Lecture%205/image.png)

**ê°€ì¤‘ì¹˜ = ì‘ì€ filter (heightì™€ widthëŠ” ì¤„ì–´ë“¤ì§€ë§Œ depthëŠ” ìœ ì§€)**

â†’ í•„í„°ë¥¼ ì´ë¯¸ì§€ì™€ í•©ì„±ê³±í•œë‹¤ = ëª¨ë“  ê³µê°„ ìœ„ì¹˜ì— ëŒ€í•´ dot productë¥¼ ê³„ì‚° = í•„ë”ë¥¼ ì´ë¯¸ì§€ì— ê²¹ì¹˜ê³  ê° ì„±ë¶„ë¼ë¦¬ ê³±í•œ ê²ƒì˜ í•©ì„ ê³„ì‚°

![image.png](assets/Lecture%205/image%201.png)

![image.png](assets/Lecture%205/image%202.png)

**Activation map**: í•„í„°ë¥¼ ê° ìœ„ì¹˜ë§ˆë‹¤ ìŠ¬ë¼ì´ë“œí•˜ë©° ê³±í•œ ê²°ê³¼ë“¤

â†’ í•´ë‹¹ ë ˆì´ì–´ì—ì„œ ì‚¬ìš©í•˜ëŠ” í•„í„° ê°œìˆ˜ë§Œí¼ activation mapì˜ depthì´ ë¨

![image.png](assets/Lecture%205/image%203.png)

**ConvNet**ì€ activation functionì´ ì‚¬ì´ì— ë°°ì¹˜ëœ ì¼ë ¨ì˜ Convolutional Layerë“¤ì˜ ì—°ì†

![image.png](assets/Lecture%205/image%204.png)

í•™ìŠµì´ ëë‚˜ë©´, ê° ë ˆì´ì–´ì˜ ì—¬ëŸ¬ í•„í„°ë“¤ì€ ê³„ì¸µì ì¸ í•™ìŠµ ê²°ê³¼ë¥¼ ê°€ì§

- ì• ë ˆì´ì–´: Low-level íŠ¹ì§• (e.g. edges)
- ì¤‘ê°„ ë ˆì´ì–´: Middle-level íŠ¹ì§• (e.g. corners, blobs)
- ë’· ë ˆì´ì–´: High-level íŠ¹ì§• (e.g. resemble concepts)

<aside>
ğŸ’¡

**ì´ ê²°ê³¼ëŠ” ConvNetì—ê²Œ ëª…ì‹œì ìœ¼ë¡œ íŠ¹ì§• í•™ìŠµì„ ê°•ì œí•˜ì§€ ì•Šì•˜ìŒì—ë„
ì‹¤ì œ ìƒë¬¼ ì‹œê°ì˜ simple cell â†’ complex cellì˜ ì—­í• ì— ëŒ€í•œ Hubel & Wieselì˜ ì—°êµ¬ê²°ê³¼ì™€ ì¼ì¹˜í•¨**

</aside>

> **Convolution (í•©ì„±ê³±)**
> 
> 
> Signal processingì—ì„œ í•©ì„±ê³±ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë¨
> 
> $$
> f[x,y]\ast g[x,y]=\sum_{n_1=-\infty}^{\infty}\sum_{n_2=-\infty}^{\infty}f[n_1,n_2]\;\cdot\;g[x-n_1,\,y-n_2]
> $$
> 
> ìœ„ì˜ ê³¼ì •ì´ í•©ì„±ê³±ê³¼ ë³¸ì§ˆì´ ê°™ê¸° ë•Œë¬¸ì— Convolutional Neural Networkë¡œ ë¶ˆë¦¬ëŠ” ê²ƒ
> 

**ì „ì²´ì ì¸ Convolutional Neural Networkì˜ ìƒê¹€ìƒˆ**

image â†’ CONV â†’ RELU â†’ CONV â†’ RELU â†’ POOL â†’ â€¦ â†’ FC

- POOL: Activation mapì˜ í¬ê¸°ë¥¼ downsampleí•˜ëŠ” ë ˆì´ì–´
- FC: ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ ëª¨ë“  ì¶œë ¥ê³¼ ì—°ê²°ëœ fully connected layerë¥¼ í™œìš©í•œ ìµœì¢… score í•¨ìˆ˜

## Spatial dimensions

Stride: í•„í„°ë¥¼ ì´ë™ì‹œí‚¤ëŠ” ê°„ê²©

- 7x7 input (spatially), assume 3x3 filter
    
    ![image.png](assets/Lecture%205/image%205.png)
    
    - stride = 1 â†’ 5x5 output
    - stride = 2 â†’ 3x3 output
    - stride = 3 â†’ doesnâ€™t fit
    â†’ ì´ë ‡ê²Œ í•©ì„±ê³±í•˜ì§€ ì•ŠìŒ. asymmetric ouputìœ¼ë¡œ ì´ì–´ì§€ê¸° ë•Œë¬¸

### Output size ê³µì‹

$$
\text{Output size}=\frac{N - F}{\text{stride}}+ 1
$$

### Zero pad

input sizeë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ ê°€ì¥ìë¦¬ë¥¼ 0ìœ¼ë¡œ ë‘ë¥´ê¸°ë„ í•¨

â€» ê´€ë ¨ ì—†ëŠ” íŠ¹ì§•ì´ ìƒê¸¸ ê°€ëŠ¥ì„± ìˆìŒ. êµ³ì´ 0ì´ ì•„ë‹ˆë”ë¼ë„ ë‹¤ë¥¸ íŒ¨ë”© ë°©ì‹ì´ ìˆìŒ.

ì¼ë°˜ì ìœ¼ë¡œ CONV layersëŠ” stride 1, FxF ì‚¬ì´ì¦ˆì˜ filterë“¤ , ê·¸ë¦¬ê³  (F-1)/2ì˜ zero-paddingìœ¼ë¡œ ì´ë¤„ì§.

â†’ ê³µê°„ì ìœ¼ë¡œ ì‚¬ì´ì¦ˆë¥¼ ìœ ì§€ (shrink ë°©ì§€)

### Summary

- **ì…ë ¥ ë³¼ë¥¨ í¬ê¸°:** W1 x H1 x D1

- **í•„ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° (4ê°€ì§€):**
    - í•„í„° ê°œìˆ˜: K
    - í•„í„° ê³µê°„ì  í¬ê¸°(spatial extent): F
    - Stride: S
    - Zero-padding: P

- ì¼ë°˜ì  ì„¤ì •
    - K = 2ì˜ ê±°ë“­ì œê³±
    - F = 1, 3, 5
    - S = 1, 2
    - P = 0, 1, 2, whatever fits
- **ì¶œë ¥ ë³¼ë¥¨ í¬ê¸°:** W2 x H2 x D2
    
    $$
    \begin{align*}
    & W_2 \;=\; \frac{W_1 - F + 2P}{S} \;+\, 1 &&\\
    & H_2 \;=\; \frac{H_1 - F + 2P}{S} \;+\, 1 &&\\
    & D_2 \;=\; K && \end{align*}
    $$
    
- **íŒŒë¼ë¯¸í„° ê³µìœ  ì‹œ ê°€ì¤‘ì¹˜ ìˆ˜:**
    - í•„í„°ë‹¹:
        
        $$
        F \times F \times D_1
        $$
        
    - ì „ì²´(í•„í„° ê°œìˆ˜, ë°”ì´ì–´ìŠ¤ ê³ ë ¤):
        
        $$
        (F \times F \times D_1)\times K + K
        $$
        
- ì¶œë ¥ ë³¼ë¥¨ì—ì„œ, **d-th depth silce**(W2 x H2)ëŠ” dë²ˆì§¸ í•„í„°ì™€ ì…ë ¥ ë³¼ë¥¨ì— stride Së¡œ ìœ íš¨í•œ **í•©ì„±ê³±**ì„ ìˆ˜í–‰í•˜ê³  dë²ˆì§¸ ë°”ì´ì–´ìŠ¤ë¥¼ ë”í•œ ê²°ê³¼ì„

## Brain/Neuron view of CONV Layer

![image.png](assets/Lecture%205/image%206.png)

- ê° ê³µê°„ì  ìœ„ì¹˜ì˜ í•„í„°ëŠ” í•´ë‹¹ ìœ„ì¹˜ì˜ ë‰´ëŸ°ì´ ì–¼ë§ˆë‚˜ í™œì„±í™”ë˜ì—ˆëŠ”ì§€ë¥¼ ì˜ë¯¸
    - **ìˆ˜ìš© ì˜ì—­(Receptive Field)**: ê° í•„í„°ê°€ ë°”ë¼ë³´ëŠ” ì‚¬ì´ì¦ˆ
    (e.g. 5x5 filter = 5x5 receptive field for each neuron)
- ì—¬ëŸ¬ ì…ë ¥ì— ëŒ€í•´ í•˜ë‚˜ì˜ ê°’ì„ ì¶œë ¥í•˜ëŠ” ê²ƒì€ Fully-connected layerì™€ ê°™ì§€ë§Œ, ëª¨ë“  ì…ë ¥ê³¼ ì—°ê²°ë˜ì§€ ì•Šì•„ local connectivityë¥¼ ê°€ì§„ë‹¤ëŠ” ì ì´ ë‹¤ë¦„
    - Fully connected layerì—ì„œëŠ” ê° ë‰´ëŸ°ì´ ëª¨ë“  ì…ë ¥ ë³¼ë¥¨ì„ ë°”ë¼ë´„
- Activation mapì˜ ë™ì¼í•œ ìœ„ì¹˜(W, H), ë‹¤ë¥¸ ê¹Šì´(D)ì˜ ê°’ì€ ë‹¤ë¥¸ ê²ƒì„ ì°¾ëŠ” ì—¬ëŸ¬ ê°œì˜ ë‹¤ë¥¸ ë‰´ë ¨ì´ ê°™ì€ ì˜ì—­(region)ì„ ë°”ë¼ë³´ëŠ” ê²ƒì„ ì˜ë¯¸
(ì´ë¯¸ì§€ì˜ ê°™ì€ ì˜ì—­ì— ëŒ€í•´ ë‹¤ë¥¸ í•„í„°ë¥¼ ì ìš©)

## Pooling Layer

![image.png](assets/Lecture%205/image%207.png)

ë” ì‘ê³  ê´€ë¦¬í•˜ê¸° ì‰¬ìš´ í‘œí˜„(representation)ì„ ë§Œë“¦

â†’ ì£¼ì–´ì§„ ì˜ì—­ì—ì„œ ë¶ˆë³€ì„±ì„ ìˆ˜í–‰í•˜ê³  ë§¤ê°œë³€ìˆ˜ ê°œìˆ˜ë¥¼ ì¤„ì„

= ê³µê°„ì ìœ¼ë¡œ(spatially) ë‹¤ìš´ìƒ˜í”Œ(downsample) â‡’ depthëŠ” ìœ ì§€

### Max Pooling

![image.png](assets/Lecture%205/image%208.png)

í•„í„°ë¥¼ ìŠ¬ë¼ì´ë“œí•˜ë©° ìµœëŒ“ê°’ì„ êµ¬í•¨

### Summary

- **ì…ë ¥ ë³¼ë¥¨ í¬ê¸°:** W1 x H1 x D1

- **í•„ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° (4ê°€ì§€):**
    - í•„í„° ê³µê°„ì  í¬ê¸°(spatial extent): F
    - Stride: S

- ì¼ë°˜ì  ì„¤ì •
    - F = 2, 3
    - S = 2
- **ì¶œë ¥ ë³¼ë¥¨ í¬ê¸°:** W2 x H2 x D2
    
    $$
    \begin{align*}
    & W_2 \;=\; \frac{W_1 - F}{S} \;+\, 1 &&\\
    & H_2 \;=\; \frac{H_1 - F}{S} \;+\, 1 &&\\
    & D_2 \;=\; D_1 && \end{align*}
    $$
    
- ê°€ì¤‘ì¹˜ ìˆ˜ = 0: ì…ë ¥ì— ëŒ€í•´ ê³ ì •ëœ í•¨ìˆ˜ë¥¼ ê³„ì‚°í•˜ê¸° ë•Œë¬¸
- ì¼ë°˜ì ìœ¼ë¡œ zero-padding ì‚¬ìš© ì•ˆí•¨: ì§ì ‘ì ì¸ ë‹¤ìš´ìƒ˜í”Œë§ì„ í•˜ê³ ì í•˜ê¸° ë•Œë¬¸

## Fully Connected Layer (FC layer)

Convolutional network outputì„ ë°›ì€ í›„, ì¼ë°˜ì ì¸ ì‹ ê²½ë§ì²˜ëŸ¼ ë³¼ë¥¨ì„ í¼ì³(stretch) ëª¨ë“  ì„±ë¶„ê³¼ ì—°ê²°

ì´ì „ ë ˆì´ì–´ì—ì„œëŠ” ê³µê°„ì  êµ¬ì¡°ë¥¼ ìœ ì§€í–ˆì§€ë§Œ, ë§ˆì§€ë§‰ ë ˆì´ì–´ì—ì„œëŠ” ì´ë“¤ì„ ëª¨ë‘ í†µí•©í•´ ì¶”ë¡ í•˜ì—¬ ì ìˆ˜ë¥¼ ì¶œë ¥

# Summary

- ConvNetsëŠ” CONV, POOL, FC ë ˆì´ì–´ë“¤ì„ í¬ê°  ê²ƒ
- í•„í„°ë¥¼ ì‘ê²Œ, ì•„í‚¤í…ì²˜ë¥¼ ê¹Šê²Œ í•˜ëŠ” ì¶”ì„¸
- POOL, FC ë ˆì´ì–´ë¥¼ ì—†ì• ê³  CONV ë ˆì´ì–´ë§Œ ì‚¬ìš©í•˜ëŠ” ì¶”ì„¸
- ì „í˜•ì ì¸ ì•„í‚¤í…ì²˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ
    
    $$
    
    [(\text{CONV} - \text{RELU}) \times N - \text{POOL?}] \times M - (\text{FC}-\text{RELU}) \times K \text{, SOFTMAX}
    $$
    
    Nì€ ì¼ë°˜ì ìœ¼ë¡œ ~5 ê¹Œì§€, Mì€ í¼, 0 â‰¤ K â‰¤ 2.
    
- ê·¸ëŸ¬ë‚˜, ResNet/GoogLeNetê³¼ ê°™ì€ ìµœê·¼ì˜ ë°œì „ì€ ì´ëŸ¬í•œ íŒ¨ë”ë‹¤ì„ì„ ë³€í™”ì‹œí‚´